Project Timeline
Week 1: Setup, Data Collection & Preliminary Feature Exploration (20 hours)
- AWS & Environment Setup: Configure AWS account and select instance types; set up storage and code repositories.
- Data Collection: Retrieve datasets from online repositories and perform an initial data audit.
- Preliminary Feature Brainstorming: Identify candidate features (e.g., binary encoding, modular residues, Hamming weight, ECPP-based signatures).
- Initial Experimentation: Write simple scripts to extract and visualize features on small data samples.
Week 2: Implementing Feature Engineering (20 hours)
- Feature Extraction Implementation: Develop and test modules for each feature extraction technique.
- Feature Validation: Run exploratory analysis to assess feature quality and relevance.
- Iteration & Refinement: Adjust feature extraction code based on early performance and domain insights.
- Documentation: Record the design decisions and any observed challenges or opportunities.
Week 3: Baseline Architecture Setup & Early Model Testing (20 hours)
- Baseline Model Development: Implement simple models (e.g., feed-forward neural network or LSTM) using the engineered features.
- Initial Model Training: Conduct short training runs on a subset of the data to verify that the features are useful.
- Performance Evaluation: Assess accuracy, convergence behavior, and computational cost.
- Identify Bottlenecks: Determine whether the features or model architectures need adjustments.
Week 4: Experimenting with Advanced Architectures (20 hours)
- Explore Transformer-Based Models: Develop a transformer-based model tailored to the data’s structure.
- Hybrid Model Testing: Implement and test combinations of convolutional and recurrent layers.
- Adversarial Approach: Initiate early experiments with a GAN setup to refine candidate prime predictions.
- Comparative Analysis: Run side-by-side comparisons of the different architectures using consistent metrics.
Week 5: Iterative Tuning & In-Depth Testing (20 hours)
- Hyperparameter Tuning: Use grid search or similar techniques to optimize model parameters for each architecture.
- Extended Training Runs: Run longer sessions on your distributed setup to evaluate convergence over time.
- Feature Impact Analysis: Determine which features contribute most to model performance; refine as needed.
- Performance Benchmarking: Compare results against baseline performance and traditional methods.
Week 6: Final Validation & Documentation (20 hours)
- Final Model Evaluation: Run final training sessions on the best-performing architectures and conduct thorough testing.
- Robustness & Sensitivity Analysis: Evaluate the models’ performance under different scenarios or data subsets.
- Documentation & Reporting: Compile a detailed report on your feature engineering process, architecture comparisons, and overall findings.
- Prepare Presentation Materials: Create slides or other materials summarizing your methodology and results for your faculty mentor or peers.

References
Atkin, A. O. L., & Morain, F. (1993). Elliptic curves and primality proving. Mathematics of Computation, 61(203), 29-68. DOI: 10.1090/S0025-5718-1993-1199989-X.
Barker, E., & Dang, Q. (2015). Recommendation for key management: Part 3 Application-specific key management guidance. NIST Special Publication 800-57.
Hellman, M. E. (1979). The mathematics of public-key cryptography. Scientific American, 241(2), 146-157.
Jansen, K. N. B. (2005). Neural networks following a binary approach applied to the integer prime-factorization problem. 2005 IEEE International Joint Conference on Neural Networks.
Murat, B., Kadyrov, S., & Tabarek, R. (2020). Integer prime factorization with deep learning. Journal of Cryptographic Engineering, 10(3), 201-215.
Nene, R., & Uludag, S. (2022). Machine learning approach to integer prime factorisation. Journal of Cryptology, 39(4), 1-24.
Rivest, R. L., Shamir, A., & Adleman, L. (1978). A method for obtaining digital signatures and public-key cryptosystems. Communications of the ACM, 21(2), 120-126.