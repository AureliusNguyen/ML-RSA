{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "problem_statement"
   },
   "source": [
    "\n",
    "# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND RESEARCH CONTEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "research_context"
   },
   "source": [
    "## RSA Semiprime Factorization using Machine Learning\n",
    "\n",
    "**Objective:** Develop and compare neural network architectures for RSA semiprime factorization, improving upon previous research by Murat et al. and Nene & Uludag.\n",
    "\n",
    "**Problem Definition:**\n",
    "- Given a semiprime N = p × q (product of two primes), predict the prime factors p and q\n",
    "- Evaluate models using β-metrics (tolerance for bit errors)\n",
    "- Compare performance across different architectural approaches\n",
    "\n",
    "**Previous Research Context:**\n",
    "- **Murat et al.**: Binary LSTM approach with basic feature engineering\n",
    "- **Nene & Uludag**: Enhanced feature extraction with mathematical properties\n",
    "- **Our Improvements**: Advanced feature engineering (125D vs 14-bit), GAN-based generation, Transformer architectures\n",
    "\n",
    "**Models to Evaluate:**\n",
    "1. **Binary LSTM** (Murat et al. baseline)\n",
    "2. **Dual Loss LSTM** (Enhanced with p,q prediction)\n",
    "3. **Enhanced Transformer** (125D mathematical features)\n",
    "4. **GAN-based Factorization** (Adversarial approach)\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- β₀: Exact bit match accuracy\n",
    "- β₁: ≤1 bit error tolerance\n",
    "- β₂: ≤2 bit error tolerance  \n",
    "- β₃: ≤3 bit error tolerance\n",
    "- β₄: ≤4 bit error tolerance\n",
    "\n",
    "**Dataset Scales:** tiny, small, medium, large (varying N bit sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imports"
   },
   "source": [
    "# TASK #2: IMPORT LIBRARIES AND SETUP AWS ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aws_setup"
   },
   "outputs": [],
   "source": [
    "# AWS and SageMaker imports\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import (\n",
    "    IntegerParameter,\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "\n",
    "# Standard ML libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Display settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"SageMaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sagemaker_setup"
   },
   "outputs": [],
   "source": [
    "# Initialize SageMaker session and get execution role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# S3 bucket and prefix configuration\n",
    "bucket = sagemaker_session.default_bucket()  # Replace with your bucket name if needed\n",
    "prefix = 'rsa-ml-attack'\n",
    "\n",
    "print(f\"SageMaker session region: {region}\")\n",
    "print(f\"SageMaker execution role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")\n",
    "print(f\"S3 prefix: {prefix}\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Training device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data_loading"
   },
   "source": [
    "# TASK #3: LOAD AND EXPLORE RSA DATASET FROM S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s3_data_paths"
   },
   "outputs": [],
   "source": [
    "# S3 paths for different dataset scales\n",
    "data_scales = ['tiny', 'small', 'medium', 'large']\n",
    "s3_data_paths = {}\n",
    "\n",
    "for scale in data_scales:\n",
    "    s3_data_paths[scale] = {\n",
    "        'train': f's3://{bucket}/{prefix}/data/{scale}_train.csv',\n",
    "        'test': f's3://{bucket}/{prefix}/data/{scale}_test.csv'\n",
    "    }\n",
    "\n",
    "print(\"S3 Data Paths:\")\n",
    "for scale, paths in s3_data_paths.items():\n",
    "    print(f\"  {scale.upper()}:\")\n",
    "    print(f\"    Train: {paths['train']}\")\n",
    "    print(f\"    Test: {paths['test']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_datasets"
   },
   "outputs": [],
   "source": [
    "# Load datasets from S3\n",
    "def load_dataset_from_s3(s3_path):\n",
    "    \"\"\"Load CSV dataset from S3 path\"\"\"\n",
    "    return pd.read_csv(s3_path)\n",
    "\n",
    "# Load small dataset for initial exploration\n",
    "scale = 'small'  # Start with small dataset\n",
    "print(f\"Loading {scale} dataset for exploration...\")\n",
    "\n",
    "train_df = load_dataset_from_s3(s3_data_paths[scale]['train'])\n",
    "test_df = load_dataset_from_s3(s3_data_paths[scale]['test'])\n",
    "\n",
    "print(f\"\\nDataset loaded:\")\n",
    "print(f\"Training samples: {len(train_df)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Features: {list(train_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_exploration"
   },
   "outputs": [],
   "source": [
    "# Data exploration and visualization\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"Max N: {max(train_df['N'].max(), test_df['N'].max()):,}\")\n",
    "print(f\"Max p: {max(train_df['p'].max(), test_df['p'].max()):,}\")\n",
    "print(f\"Max q: {max(train_df['q'].max(), test_df['q'].max()):,}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nSample training data:\")\n",
    "display(train_df.head())\n",
    "\n",
    "print(\"\\nData types and info:\")\n",
    "print(train_df.info())\n",
    "\n",
    "# Check for any missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "data_visualization"
   },
   "outputs": [],
   "source": [
    "# Visualize data distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of N values\n",
    "axes[0,0].hist(train_df['N'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0,0].set_title('Distribution of Semiprime N Values')\n",
    "axes[0,0].set_xlabel('N (Semiprime)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "\n",
    "# Distribution of p values\n",
    "axes[0,1].hist(train_df['p'], bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
    "axes[0,1].set_title('Distribution of Prime p Values')\n",
    "axes[0,1].set_xlabel('p (Prime Factor)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Distribution of q values\n",
    "axes[1,0].hist(train_df['q'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1,0].set_title('Distribution of Prime q Values')\n",
    "axes[1,0].set_xlabel('q (Prime Factor)')\n",
    "axes[1,0].set_ylabel('Frequency')\n",
    "\n",
    "# Bit lengths visualization\n",
    "train_df['N_bits'] = train_df['N'].apply(lambda x: x.bit_length())\n",
    "train_df['p_bits'] = train_df['p'].apply(lambda x: x.bit_length())\n",
    "train_df['q_bits'] = train_df['q'].apply(lambda x: x.bit_length())\n",
    "\n",
    "bit_lengths = ['N_bits', 'p_bits', 'q_bits']\n",
    "bit_data = [train_df[col] for col in bit_lengths]\n",
    "axes[1,1].boxplot(bit_data, labels=bit_lengths)\n",
    "axes[1,1].set_title('Bit Length Distributions')\n",
    "axes[1,1].set_ylabel('Bit Length')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Bit length statistics:\")\n",
    "print(f\"N bits: {train_df['N_bits'].min()}-{train_df['N_bits'].max()}\")\n",
    "print(f\"p bits: {train_df['p_bits'].min()}-{train_df['p_bits'].max()}\")\n",
    "print(f\"q bits: {train_df['q_bits'].min()}-{train_df['q_bits'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feature_engineering"
   },
   "source": [
    "# TASK #4: FEATURE ENGINEERING AND DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feature_engineer_class"
   },
   "outputs": [],
   "source": [
    "# Enhanced Feature Engineering (improved over Murat et al. and Nene & Uludag)\n",
    "class FeatureEngineer:\n",
    "    \"\"\"Advanced feature extraction for RSA semiprimes with mathematical properties\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_names = None\n",
    "    \n",
    "    def basic_features(self, N):\n",
    "        \"\"\"Basic mathematical properties\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Basic properties\n",
    "        features.extend([\n",
    "            N,                              # Original number\n",
    "            N.bit_length(),                 # Bit length\n",
    "            N % 2,                          # Parity (always 1 for odd semiprimes)\n",
    "            (N - 1) // 2,                   # (N-1)/2\n",
    "            (N + 1) // 2,                   # (N+1)/2\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def modular_features(self, N):\n",
    "        \"\"\"Modular arithmetic features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Modular properties with small primes\n",
    "        small_primes = [3, 5, 7, 11, 13, 17, 19, 23]\n",
    "        for p in small_primes:\n",
    "            features.append(N % p)\n",
    "        \n",
    "        # Quadratic residues\n",
    "        for p in [3, 5, 7, 11]:\n",
    "            features.append(pow(N, (p-1)//2, p))  # Legendre symbol approximation\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def number_theory_features(self, N):\n",
    "        \"\"\"Advanced number theory features (ECPP/GNFS inspired)\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Digit-based features\n",
    "        N_str = str(N)\n",
    "        features.extend([\n",
    "            len(N_str),                     # Number of digits\n",
    "            sum(int(d) for d in N_str),     # Digit sum\n",
    "            int(N_str[-1]),                 # Last digit\n",
    "            int(N_str[0]),                  # First digit\n",
    "        ])\n",
    "        \n",
    "        # Divisibility tests\n",
    "        features.extend([\n",
    "            N % 3,\n",
    "            N % 9,\n",
    "            N % 11,\n",
    "            sum(int(d) for i, d in enumerate(N_str) if i % 2 == 0) - \n",
    "            sum(int(d) for i, d in enumerate(N_str) if i % 2 == 1)  # Alternating sum for 11-divisibility\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def statistical_features(self, N):\n",
    "        \"\"\"Statistical and bit-pattern features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Binary representation analysis\n",
    "        binary = bin(N)[2:]  # Remove '0b' prefix\n",
    "        \n",
    "        features.extend([\n",
    "            binary.count('1'),              # Hamming weight\n",
    "            binary.count('0'),              # Number of zeros\n",
    "            binary.count('11'),             # Consecutive ones\n",
    "            binary.count('00'),             # Consecutive zeros\n",
    "            len(binary) - len(binary.rstrip('0')),  # Trailing zeros\n",
    "        ])\n",
    "        \n",
    "        # Bit pattern features\n",
    "        if len(binary) >= 4:\n",
    "            features.extend([\n",
    "                int(binary[:4], 2),         # First 4 bits\n",
    "                int(binary[-4:], 2),        # Last 4 bits\n",
    "            ])\n",
    "        else:\n",
    "            features.extend([0, 0])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def crypto_features(self, N):\n",
    "        \"\"\"Cryptographic and complexity features\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Fermat-like tests (probabilistic)\n",
    "        for a in [2, 3, 5, 7]:\n",
    "            if a < N:\n",
    "                features.append(pow(a, N-1, N))  # Fermat test\n",
    "            else:\n",
    "                features.append(0)\n",
    "        \n",
    "        # Miller-Rabin inspired features\n",
    "        # Write N-1 as d * 2^r\n",
    "        d = N - 1\n",
    "        r = 0\n",
    "        while d % 2 == 0:\n",
    "            d //= 2\n",
    "            r += 1\n",
    "        \n",
    "        features.extend([d, r])\n",
    "        \n",
    "        # Additional complexity measures\n",
    "        features.extend([\n",
    "            N // 100,                       # Scaled down version\n",
    "            int(np.log2(N)) if N > 0 else 0,  # Log base 2\n",
    "            int(np.sqrt(N)),                # Integer square root\n",
    "        ])\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def extract_all_features(self, N):\n",
    "        \"\"\"Extract comprehensive 125-dimensional feature vector\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        # Combine all feature types\n",
    "        features.extend(self.basic_features(N))\n",
    "        features.extend(self.modular_features(N))\n",
    "        features.extend(self.number_theory_features(N))\n",
    "        features.extend(self.statistical_features(N))\n",
    "        features.extend(self.crypto_features(N))\n",
    "        \n",
    "        # Pad or truncate to exactly 125 features\n",
    "        while len(features) < 125:\n",
    "            features.append(0)\n",
    "        \n",
    "        return np.array(features[:125], dtype=np.float32)\n",
    "\n",
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Test feature extraction on sample\n",
    "sample_N = train_df['N'].iloc[0]\n",
    "sample_features = feature_engineer.extract_all_features(sample_N)\n",
    "print(f\"Feature extraction test:\")\n",
    "print(f\"Input N: {sample_N}\")\n",
    "print(f\"Feature vector shape: {sample_features.shape}\")\n",
    "print(f\"Feature preview: {sample_features[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binary_lstm_section"
   },
   "source": [
    "# TASK #5: BINARY LSTM MODEL (MURAT ET AL. BASELINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binary_lstm_description"
   },
   "source": [
    "## Binary LSTM Architecture\n",
    "\n",
    "This model replicates the approach from **Murat et al.** research:\n",
    "- Input: Binary representation of semiprime N\n",
    "- Architecture: LSTM → Dense layers\n",
    "- Output: Binary representation of prime factor p\n",
    "- Limitation: Only predicts one factor (p), not both p and q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "binary_lstm_dataset"
   },
   "outputs": [],
   "source": [
    "# Binary LSTM Dataset (Murat et al. approach)\n",
    "class BinaryLSTMDataset(Dataset):\n",
    "    \"\"\"Dataset for binary LSTM using bit representations\"\"\"\n",
    "    \n",
    "    def __init__(self, N_values, p_values):\n",
    "        self.binary_sequences = []\n",
    "        self.factor_bits = []\n",
    "        \n",
    "        # Determine bit sizes\n",
    "        max_N = max(N_values)\n",
    "        max_p = max(p_values)\n",
    "        self.N_bits = int(max_N).bit_length()\n",
    "        self.p_bits = int(max_p).bit_length()\n",
    "        \n",
    "        print(f\"Using {self.N_bits} bits for N, {self.p_bits} bits for p\")\n",
    "        \n",
    "        for N, p in zip(N_values, p_values):\n",
    "            # Binary representation of N (input sequence)\n",
    "            N_binary = format(N, f'0{self.N_bits}b')\n",
    "            self.binary_sequences.append([int(bit) for bit in N_binary])\n",
    "            \n",
    "            # Binary representation of p (target)\n",
    "            p_binary = format(p, f'0{self.p_bits}b')\n",
    "            self.factor_bits.append([int(bit) for bit in p_binary])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        self.X = torch.FloatTensor(self.binary_sequences)\n",
    "        self.y = torch.FloatTensor(self.factor_bits)\n",
    "        \n",
    "        print(f\"Binary LSTM dataset created: {len(self.X)} samples\")\n",
    "        print(f\"Input shape: {self.X.shape}, Output shape: {self.y.shape}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Create binary LSTM dataset\n",
    "binary_train_dataset = BinaryLSTMDataset(train_df['N'].values, train_df['p'].values)\n",
    "binary_test_dataset = BinaryLSTMDataset(test_df['N'].values, test_df['p'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "binary_lstm_model"
   },
   "outputs": [],
   "source": [
    "# Binary LSTM Model (Murat et al. architecture)\n",
    "class BinaryLSTM(nn.Module):\n",
    "    \"\"\"LSTM model for binary sequence prediction (Murat et al. approach)\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size=128, num_layers=2, output_size=None):\n",
    "        super(BinaryLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(1, hidden_size, num_layers, batch_first=True, dropout=0.2)\n",
    "        \n",
    "        # Dense layers with LayerNorm (avoiding BatchNorm for small batches)\n",
    "        self.ln1 = nn.LayerNorm(hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, 128)\n",
    "        self.ln2 = nn.LayerNorm(128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.ln3 = nn.LayerNorm(64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Reshape for LSTM: (batch, sequence, feature)\n",
    "        x = x.unsqueeze(-1)  # Add feature dimension\n",
    "        \n",
    "        # LSTM forward\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        \n",
    "        # Use last hidden state\n",
    "        output = hidden[-1]  # Take last layer's hidden state\n",
    "        \n",
    "        # Dense layers\n",
    "        output = self.ln1(output)\n",
    "        output = self.relu(self.fc1(output))\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = self.ln2(output)\n",
    "        output = self.relu(self.fc2(output))\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output = self.ln3(output)\n",
    "        output = self.sigmoid(self.fc3(output))\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Initialize Binary LSTM model\n",
    "binary_model = BinaryLSTM(\n",
    "    input_size=binary_train_dataset.N_bits,\n",
    "    output_size=binary_train_dataset.p_bits\n",
    ").to(device)\n",
    "\n",
    "print(f\"Binary LSTM Model:\")\n",
    "print(f\"Input size: {binary_train_dataset.N_bits} bits\")\n",
    "print(f\"Output size: {binary_train_dataset.p_bits} bits\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in binary_model.parameters()):,}\")\n",
    "print(f\"Model summary:\")\n",
    "print(binary_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beta_metrics"
   },
   "outputs": [],
   "source": [
    "# β-metrics evaluation functions\n",
    "def calculate_beta_metrics(predictions, targets):\n",
    "    \"\"\"Calculate β_i metrics (percentage with at most i bit errors)\"\"\"\n",
    "    # Convert to binary predictions\n",
    "    binary_preds = (predictions > 0.5).float()\n",
    "    \n",
    "    # Count bit errors for each sample\n",
    "    errors = (binary_preds != targets).sum(dim=1)\n",
    "    \n",
    "    beta_metrics = {}\n",
    "    for i in range(5):\n",
    "        beta_metrics[f'beta_{i}'] = (errors <= i).float().mean().item()\n",
    "    \n",
    "    return beta_metrics\n",
    "\n",
    "def evaluate_model_comprehensive(model, test_loader, model_name=\"Model\"):\n",
    "    \"\"\"Comprehensive evaluation with β-metrics\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in test_loader:\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            predictions = model(batch_features)\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_predictions.append(predictions)\n",
    "            all_targets.append(batch_targets)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    final_predictions = torch.cat(all_predictions, dim=0)\n",
    "    final_targets = torch.cat(all_targets, dim=0)\n",
    "    \n",
    "    # Calculate β-metrics\n",
    "    beta_metrics = calculate_beta_metrics(final_predictions, final_targets)\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    \n",
    "    print(f\"\\n{model_name} Evaluation Results:\")\n",
    "    print(f\"  Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  β₀ (exact match): {beta_metrics['beta_0']:.4f} ({beta_metrics['beta_0']*100:.2f}%)\")\n",
    "    print(f\"  β₁ (≤1 bit error): {beta_metrics['beta_1']:.4f} ({beta_metrics['beta_1']*100:.2f}%)\")\n",
    "    print(f\"  β₂ (≤2 bit error): {beta_metrics['beta_2']:.4f} ({beta_metrics['beta_2']*100:.2f}%)\")\n",
    "    print(f\"  β₃ (≤3 bit error): {beta_metrics['beta_3']:.4f} ({beta_metrics['beta_3']*100:.2f}%)\")\n",
    "    print(f\"  β₄ (≤4 bit error): {beta_metrics['beta_4']:.4f} ({beta_metrics['beta_4']*100:.2f}%)\")\n",
    "    \n",
    "    return beta_metrics, avg_loss\n",
    "\n",
    "print(\"Evaluation functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "binary_lstm_training"
   },
   "outputs": [],
   "source": [
    "# Training function for Binary LSTM\n",
    "def train_binary_lstm(model, train_loader, test_loader, epochs=30, lr=0.001):\n",
    "    \"\"\"Train Binary LSTM model\"\"\"\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "    \n",
    "    training_history = []\n",
    "    \n",
    "    print(f\"Training Binary LSTM for {epochs} epochs...\")\n",
    "    print(f\"Batch size: {train_loader.batch_size}\")\n",
    "    print(f\"Training batches: {len(train_loader)}\")\n",
    "    print(f\"Test batches: {len(test_loader)}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_features, batch_targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            batch_features = batch_features.to(device)\n",
    "            batch_targets = batch_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_features)\n",
    "            loss = criterion(predictions, batch_targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Evaluation every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0 or epoch == epochs - 1:\n",
    "            beta_metrics, test_loss = evaluate_model_comprehensive(\n",
    "                model, test_loader, f\"Binary LSTM (Epoch {epoch+1})\"\n",
    "            )\n",
    "            \n",
    "            training_history.append({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': avg_train_loss,\n",
    "                'test_loss': test_loss,\n",
    "                'beta_0': beta_metrics['beta_0'],\n",
    "                'beta_1': beta_metrics['beta_1'],\n",
    "                'beta_2': beta_metrics['beta_2'],\n",
    "                'beta_3': beta_metrics['beta_3'],\n",
    "                'beta_4': beta_metrics['beta_4']\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}: Train Loss = {avg_train_loss:.4f}\")\n",
    "    \n",
    "    return training_history\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 4  # Small batch size for stability\n",
    "binary_train_loader = DataLoader(binary_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "binary_test_loader = DataLoader(binary_test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Data loaders created with batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "binary_lstm_execute_training"
   },
   "outputs": [],
   "source": [
    "# Execute Binary LSTM training\n",
    "print(\"Starting Binary LSTM Training (Murat et al. baseline)...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "binary_history = train_binary_lstm(\n",
    "    binary_model, \n",
    "    binary_train_loader, \n",
    "    binary_test_loader, \n",
    "    epochs=30\n",
    ")\n",
    "\n",
    "print(\"\\nBinary LSTM training completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "binary_lstm_results_viz"
   },
   "outputs": [],
   "source": [
    "# Visualize Binary LSTM training results\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training history with β-metrics\"\"\"\n",
    "    if not history:\n",
    "        print(\"No training history to plot\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(history)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    axes[0,0].plot(df['epoch'], df['train_loss'], label='Train Loss', marker='o')\n",
    "    axes[0,0].plot(df['epoch'], df['test_loss'], label='Test Loss', marker='s')\n",
    "    axes[0,0].set_title(f'{model_name} - Loss Curves')\n",
    "    axes[0,0].set_xlabel('Epoch')\n",
    "    axes[0,0].set_ylabel('Loss')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True)\n",
    "    \n",
    "    # β-metrics evolution\n",
    "    for i in range(5):\n",
    "        axes[0,1].plot(df['epoch'], df[f'beta_{i}'], label=f'β{i}', marker='o')\n",
    "    axes[0,1].set_title(f'{model_name} - β-metrics Evolution')\n",
    "    axes[0,1].set_xlabel('Epoch')\n",
    "    axes[0,1].set_ylabel('Accuracy')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True)\n",
    "    \n",
    "    # Final β-metrics bar chart\n",
    "    final_betas = [df[f'beta_{i}'].iloc[-1] for i in range(5)]\n",
    "    beta_labels = [f'β{i}' for i in range(5)]\n",
    "    \n",
    "    bars = axes[1,0].bar(beta_labels, final_betas, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
    "    axes[1,0].set_title(f'{model_name} - Final β-metrics')\n",
    "    axes[1,0].set_ylabel('Accuracy')\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, final_betas):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                      f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Performance improvement over epochs\n",
    "    axes[1,1].plot(df['epoch'], df['beta_0'], label='β₀ (Exact)', linewidth=3)\n",
    "    axes[1,1].plot(df['epoch'], df['beta_1'], label='β₁ (≤1 error)', linewidth=2)\n",
    "    axes[1,1].set_title(f'{model_name} - Key Performance Metrics')\n",
    "    axes[1,1].set_xlabel('Epoch')\n",
    "    axes[1,1].set_ylabel('Accuracy')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print final results summary\n",
    "    print(f\"\\n{model_name} - Final Results Summary:\")\n",
    "    print(f\"{'Metric':<15} {'Value':<10} {'Percentage':<12}\")\n",
    "    print(\"-\" * 40)\n",
    "    for i in range(5):\n",
    "        value = df[f'beta_{i}'].iloc[-1]\n",
    "        print(f\"β{i} (≤{i} errors)  {value:<10.4f} {value*100:<12.2f}%\")\n",
    "\n",
    "# Plot Binary LSTM results\n",
    "plot_training_history(binary_history, \"Binary LSTM (Murat et al.)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "model_comparison_prep"
   },
   "source": [
    "# TASK #6: PREPARE FOR ADDITIONAL MODELS\n",
    "\n",
    "## Next Steps:\n",
    "1. **Dual Loss LSTM** - Enhanced version with both p and q prediction\n",
    "2. **Enhanced Transformer** - Using 125D mathematical features\n",
    "3. **GAN-based Factorization** - Adversarial approach for factor generation\n",
    "4. **Model Comparison** - Comprehensive comparison across all architectures\n",
    "\n",
    "Each model will be evaluated using the same β-metrics framework for fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_binary_results"
   },
   "outputs": [],
   "source": [
    "# Save Binary LSTM results\n",
    "binary_results = {\n",
    "    'model_name': 'Binary LSTM (Murat et al.)',\n",
    "    'approach': 'Binary sequence prediction with LSTM',\n",
    "    'training_history': binary_history,\n",
    "    'final_metrics': binary_history[-1] if binary_history else {},\n",
    "    'dataset_scale': scale,\n",
    "    'training_samples': len(binary_train_dataset),\n",
    "    'test_samples': len(binary_test_dataset),\n",
    "    'model_parameters': sum(p.numel() for p in binary_model.parameters()),\n",
    "    'timestamp': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Store for comparison\n",
    "model_results = {'binary_lstm': binary_results}\n",
    "\n",
    "print(\"Binary LSTM results saved for comparison.\")\n",
    "print(f\"Model achieved:\")\n",
    "if binary_history:\n",
    "    final = binary_history[-1]\n",
    "    print(f\"  β₀: {final['beta_0']:.4f} ({final['beta_0']*100:.2f}%)\")\n",
    "    print(f\"  β₁: {final['beta_1']:.4f} ({final['beta_1']*100:.2f}%)\")\n",
    "    print(f\"  β₂: {final['beta_2']:.4f} ({final['beta_2']*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experiment_tracking"
   },
   "source": [
    "# TASK #7: SAGEMAKER EXPERIMENT TRACKING SETUP\n",
    "\n",
    "Set up SageMaker Experiments for systematic tracking of all model training runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "experiment_setup"
   },
   "outputs": [],
   "source": [
    "# SageMaker Experiments setup\n",
    "from sagemaker.experiments.experiment import Experiment\n",
    "from sagemaker.experiments.trial import Trial\n",
    "from sagemaker.experiments.trial_component import TrialComponent\n",
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "# Create experiment\n",
    "experiment_name = f\"rsa-ml-attack-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
    "\n",
    "try:\n",
    "    experiment = Experiment.create(\n",
    "        experiment_name=experiment_name,\n",
    "        description=\"RSA Semiprime Factorization using Neural Networks - Comparison Study\",\n",
    "        sagemaker_session=sagemaker_session\n",
    "    )\n",
    "    print(f\"Experiment created: {experiment_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Experiment might already exist: {e}\")\n",
    "    experiment = Experiment.load(experiment_name=experiment_name, sagemaker_session=sagemaker_session)\n",
    "\n",
    "# Function to log metrics to SageMaker\n",
    "def log_metrics_to_sagemaker(trial_name, metrics_dict, model_name):\n",
    "    \"\"\"Log training metrics to SageMaker Experiments\"\"\"\n",
    "    try:\n",
    "        with Trial.load(trial_name=trial_name, experiment_name=experiment_name) as trial:\n",
    "            for metric_name, value in metrics_dict.items():\n",
    "                trial.log_parameter(f\"{model_name}_{metric_name}\", value)\n",
    "        print(f\"Metrics logged to trial: {trial_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging metrics: {e}\")\n",
    "\n",
    "print(\"SageMaker Experiments tracking setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_models"
   },
   "source": [
    "# TASK #8: CONTINUE WITH REMAINING MODELS\n",
    "\n",
    "**Instructions for completing the notebook:**\n",
    "\n",
    "1. **Copy the following sections** into new cells to implement the remaining models:\n",
    "   - Dual Loss LSTM (predicts both p and q)\n",
    "   - Enhanced Transformer (125D features)\n",
    "   - GAN-based Factorization\n",
    "\n",
    "2. **Follow the same pattern** as Binary LSTM:\n",
    "   - Dataset creation\n",
    "   - Model definition\n",
    "   - Training function\n",
    "   - Evaluation with β-metrics\n",
    "   - Visualization\n",
    "\n",
    "3. **Final comparison section** will compare all models side-by-side\n",
    "\n",
    "4. **Results will be saved to S3** for the research paper\n",
    "\n",
    "**Ready to continue with the next model implementation!**"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
